{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# transfer_fs\n",
    "### Import Python Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.backend import clear_session\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.python.keras.utils.layer_utils import print_summary\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "import numpy as np\n",
    "from CLRNet_convlstm import CLRNet\n",
    "from Utility_functions import create_sequence\n",
    "from datetime import datetime as dt\n",
    "from DFVDSequence import DFVDSequence\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"yolo\")\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "# import shutil\n",
    "\n",
    "# from src.xception_convlstm import Xception\n",
    "# print(cv2.__version__)\n",
    "# import random\n",
    "# random.seed(32)\n",
    "\n",
    "dataset_dir='DeepFakeDatasetReal'\n",
    "# from src.cl_basic import cl_basic\n",
    "import ipykernel\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# Choose GPU NUMBERS [0, 1, 2, 3]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DeepFake Transfer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "train_video_per_batch=16\n",
    "val_video_per_batch=16\n",
    "test_video_per_batch=16\n",
    "frames_per_video=80 #total frames\n",
    "frames_per_video_per_batch=5 #frames in one batch\n",
    "image_size=240\n",
    "channel=3\n",
    "\n",
    "X_train,y_train, class_weights_train =create_sequence('datasets/TransferLearning/DeepFake/train/',frames_per_video_per_batch,frames_per_video)\n",
    "X_val,y_val,class_weights_val=create_sequence('datasets/TransferLearning/DeepFake/val/',frames_per_video_per_batch,frames_per_video)\n",
    "train_it=DFVDSequence(X_train,y_train,train_video_per_batch,frames_per_video_per_batch,image_size,True)\n",
    "val_it=DFVDSequence(X_val,y_val,val_video_per_batch,frames_per_video_per_batch,image_size,False)\n",
    "\n",
    "fig=plt.figure(figsize=(10, 100))\n",
    "columns = 4\n",
    "rows = 23\n",
    "print(\"Training\")\n",
    "x=1\n",
    "temp=train_it.__getitem__(1)\n",
    "fig.add_subplot(rows, columns, x)\n",
    "print(\"Min:\",np.array(temp[0][0,0]).min(),\"Max:\",np.array(temp[0][0,0]).max())\n",
    "print(\"Min:\",np.array(temp[0][1,0]).min(),\"Max:\",np.array(temp[0][1,0]).max())\n",
    "plt.imshow(np.array(temp[0][0,0]))\n",
    "fig.add_subplot(rows, columns, x+1)\n",
    "plt.imshow(np.array(temp[0][1,0]))\n",
    "print(temp[0].shape,temp[1].shape,train_it.dataset_size,train_it.frame_counter,train_it.__len__(),temp[1][0],temp[1][1])\n",
    "# x+=2\n",
    "plt.title(\"Training Set Examples\")        \n",
    "plt.show()\n",
    "train_it.on_epoch_end()\n",
    "\n",
    "fig=plt.figure(figsize=(10, 100))\n",
    "columns = 4\n",
    "rows = 23\n",
    "print(\"Validation\")\n",
    "x=1\n",
    "temp=val_it.__getitem__(1)\n",
    "fig.add_subplot(rows, columns, x)\n",
    "print(\"Min:\",np.array(temp[0][0,0]).min(),\"Max:\",np.array(temp[0][0,0]).max())\n",
    "print(\"Min:\",np.array(temp[0][1,0]).min(),\"Max:\",np.array(temp[0][1,0]).max())\n",
    "plt.imshow(np.array(temp[0][0,0]),)\n",
    "fig.add_subplot(rows, columns, x+1)\n",
    "plt.imshow(np.array(temp[0][1,0]))\n",
    "print(temp[0].shape,temp[1].shape,val_it.dataset_size,val_it.frame_counter,val_it.__len__(),temp[1][0],temp[1][1])\n",
    "plt.title(\"Validation Set Examples\")   \n",
    "plt.show()\n",
    "val_it.on_epoch_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# model=CLRNet(input_shape=(frames_per_video_per_batch,image_size, image_size, channel), classes=2, block='bottleneck', residual_unit='v2',\n",
    "#            repetitions=[3, 4, 6, 3], initial_filters=64, activation='softmax', include_top=False,\n",
    "#            input_tensor=None, dropout=0.25, transition_dilation_rate=(1, 1),\n",
    "#            initial_strides=(2, 2), initial_kernel_size=(7, 7), initial_pooling='max',\n",
    "#            final_pooling=None, top='classification')\n",
    "# print_summary(model, line_length=150, positions=None, print_fn=None)\n",
    "model = load_model('models/FaceSwap/CLRNet50(bk)/19-12-17-10_03/CLRNet50(bk)_19-12-17-10_03-130-0.99.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# layers = [(layer, layer.name, layer.trainable) for layer in model.layers]\n",
    "# count =0\n",
    "# print(len(model.layers))\n",
    "# for layer in model.layers:\n",
    "#     count+=1\n",
    "#     if count==len(model.layers):\n",
    "#         continue\n",
    "#     else:\n",
    "#         layer.trainable = False\n",
    "# print(layers[206])\n",
    "\n",
    "\n",
    "# # Keep a copy of the original class\n",
    "# BatchNormalization = layers.BatchNormalization\n",
    "\n",
    "# # Patch the class temporarily\n",
    "# layers.BatchNormalization = FrozenBatchNormalization\n",
    "\n",
    "# set_trainable = False\n",
    "# for layer in model.layers:\n",
    "\n",
    "#     if layer.name in ['res5c_branch2b', 'res5c_branch2c', 'activation_21']:\n",
    "#         set_trainable = True\n",
    "#     if set_trainable:\n",
    "#         layer.trainable = True\n",
    "#     else:\n",
    "#         layer.trainable = False\n",
    "#     if 'batch_normalization' in layer.name:\n",
    "#         print(layer.name)\n",
    "#         layer.trainable = False\n",
    "from tensorflow.python.keras import layers\n",
    "class FrozenBatchNormalization(layers.BatchNormalization):\n",
    "    def call(self, inputs, training=None):\n",
    "        return super().call(inputs=inputs, training=False)\n",
    "mode ='training'\n",
    "top_k_layers=140\n",
    "\n",
    "# model.trainable = True\n",
    "if mode == 'training':\n",
    "    _bottom_layers = model.layers[:-top_k_layers]\n",
    "    _top_layers = model.layers[-top_k_layers:]\n",
    "elif mode == 'inference':\n",
    "    _bottom_layers = model.layers\n",
    "    _top_layers = []\n",
    "\n",
    "for _layer in _bottom_layers:\n",
    "    _layer.trainable = False\n",
    "    if 'batch_normalization' in _layer.name:\n",
    "        print('Freezing BN layers ... {}'.format(_layer.name))\n",
    "        _layer = FrozenBatchNormalization\n",
    "\n",
    "for _layer in _top_layers:\n",
    "    _layer.trainable = True\n",
    "    if 'batch_normalization' in _layer.name:\n",
    "        print('Unfreezing BN layers ... {}'.format(_layer.name))\n",
    "        _layer = layers.BatchNormalization\n",
    "# for layer in model.layers:\n",
    "#     if 'batch_normalization' in layer.name:\n",
    "#         print(layer)\n",
    "layers_df = [(layer, layer.name, layer.trainable) for layer in model.layers]\n",
    "pd.DataFrame(layers_df, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])\n",
    "\n",
    "# print(df.head(207))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "print_summary(model, line_length=150, positions=None, print_fn=None)\n",
    "adam_fine = Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam_fine,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "_id = dt.now().strftime(\"%y-%m-%d-%H_%M\")\n",
    "save_dir=os.path.join(os.getcwd(),'models','transfer_fs','CLRNet','DeepFake',_id)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "filepath=os.path.join(save_dir,\"{epoch:02d}-{acc:.2f}\"+\".hdf5\")\n",
    "checkpoint = ModelCheckpoint(filepath)#, monitor='acc', verbose=2, save_best_only=True, mode='max')\n",
    "csv_logger = CSVLogger(os.path.join(save_dir,\"training.csv\"), append=True, separator=',')\n",
    "# earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "# mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "\n",
    "# earlyStop=EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "# callbacks_list = [csv_logger]\n",
    "callbacks_list = [checkpoint,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_it, validation_data=val_it,epochs=100,\n",
    "                    callbacks=callbacks_list,shuffle=False,class_weight=class_weights_train,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_it, validation_data=val_it,epochs=150,initial_epoch=100,\n",
    "                    callbacks=callbacks_list,shuffle=False,class_weight=class_weights_train,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode ='inference'\n",
    "top_k_layers=140\n",
    "\n",
    "# model.trainable = True\n",
    "if mode == 'training':\n",
    "    _bottom_layers = model.layers[:-top_k_layers]\n",
    "    _top_layers = model.layers[-top_k_layers:]\n",
    "elif mode == 'inference':\n",
    "    _bottom_layers = model.layers\n",
    "    _top_layers = []\n",
    "\n",
    "for _layer in _bottom_layers:\n",
    "    _layer.trainable = False\n",
    "    if 'batch_normalization' in _layer.name:\n",
    "        print('Freezing BN layers ... {}'.format(_layer.name))\n",
    "        _layer = FrozenBatchNormalization\n",
    "\n",
    "for _layer in _top_layers:\n",
    "    _layer.trainable = True\n",
    "    if 'batch_normalization' in _layer.name:\n",
    "        print('Unfreezing BN layers ... {}'.format(_layer.name))\n",
    "        _layer = layers.BatchNormalization\n",
    "# for layer in model.layers:\n",
    "#     if 'batch_normalization' in layer.name:\n",
    "#         print(layer)\n",
    "layers_df = [(layer, layer.name, layer.trainable) for layer in model.layers]\n",
    "pd.DataFrame(layers_df, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(FaceSwap_it, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = FaceSwap_it.pre_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_labels = list(FaceSwap_it.class_indices.keys()) \n",
    "report = classification_report(true_classes, predicted_classes, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(FaceSwap_it.pre_classes)\n",
    "# print(len(predictions))\n",
    "# print(len(y_FaceSwap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFakes Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DeepFakes Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_video_per_batch=5\n",
    "frames_per_video=80\n",
    "image_size=240\n",
    "test_video_per_batch=16\n",
    "data_augmentation=False\n",
    "X_DeepFake,y_DeepFake,class_weights_DeepFake=create_sequence('datasets/DeepFake/test/',frames_per_video_per_batch,frames_per_video)\n",
    "DeepFake_it=DFVDSequence(X_DeepFake,y_DeepFake,test_video_per_batch,frames_per_video_per_batch,image_size,data_augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict DeepFakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(DeepFake_it, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = DeepFake_it.pre_classes\n",
    "# class_labels = list(DeepFake_it.class_indices.keys()) \n",
    "report = classification_report(true_classes, predicted_classes,  digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FaceSwap Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load FaceSwap Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_video_per_batch=5\n",
    "frames_per_video=80\n",
    "image_size=240\n",
    "test_video_per_batch=16\n",
    "data_augmentation=False\n",
    "\n",
    "X_FaceSwap,y_FaceSwap,class_weights_FaceSwap=create_sequence('datasets/FaceSwap/test/',frames_per_video_per_batch,frames_per_video)\n",
    "FaceSwap_it=DFVDSequence(X_FaceSwap,y_FaceSwap,test_video_per_batch,frames_per_video_per_batch,image_size,data_augmentation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict FaceSwap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(FaceSwap_it, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = FaceSwap_it.pre_classes\n",
    "# class_labels = list(FaceSwap_it.class_indices.keys()) \n",
    "report = classification_report(true_classes, predicted_classes, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face2Face Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Face2Face Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_video_per_batch=5\n",
    "frames_per_video=80\n",
    "image_size=240\n",
    "test_video_per_batch=16\n",
    "data_augmentation=False\n",
    "X_Face2Face,y_Face2Face,class_weights_Face2Face=create_sequence('datasets/Face2Face/test/',frames_per_video_per_batch,frames_per_video)\n",
    "Face2Face_it=DFVDSequence(X_Face2Face,y_Face2Face,test_video_per_batch,frames_per_video_per_batch,image_size,data_augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Face2Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(Face2Face_it, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = Face2Face_it.pre_classes\n",
    "# class_labels = list(Face2Face_it.class_indices.keys()) \n",
    "report = classification_report(true_classes, predicted_classes, digits=4)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}