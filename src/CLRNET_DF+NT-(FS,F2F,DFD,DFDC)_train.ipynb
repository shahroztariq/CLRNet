{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CLRNET_DF+NT-(FS,F2F,DFD,DFDC)_train\n",
    "### Import Python Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.backend import clear_session\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
    "from tensorflow.python.keras.utils.layer_utils import print_summary\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "import numpy as np\n",
    "from CLRNet_convlstm import CLRNet\n",
    "from Utility_functions import create_sequence, FreezeBatchNormalization,AdditionalValidationSets\n",
    "from datetime import datetime as dt\n",
    "from DFVDSequence import DFVDSequence\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"yolo\")\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "# import shutil\n",
    "from tensorflow.python.keras import layers\n",
    "# from src.xception_convlstm import Xception\n",
    "# print(cv2.__version__)\n",
    "# import random\n",
    "# random.seed(32)\n",
    "\n",
    "dataset_dir='DeepFakeDatasetReal'\n",
    "# from src.cl_basic import cl_basic\n",
    "import ipykernel\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# Choose GPU NUMBERS [0, 1, 2, 3]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Transfer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "train_video_per_batch=40\n",
    "val_video_per_batch=25\n",
    "test_video_per_batch=25\n",
    "train_frames_per_video=80 #total frames\n",
    "val_frames_per_video=5 #total frames\n",
    "test_frames_per_video=5 #total frames\n",
    "frames_per_video_per_batch=5 #frames in one batch\n",
    "image_size=128\n",
    "channel=3\n",
    "train_augumentation=True\n",
    "val_augumentation=False\n",
    "test_augumentation=False\n",
    "X_train,y_train, class_weights_train = create_sequence(['datasets/TransferLearning/DeepFake/train/',\n",
    "                                               'datasets/TransferLearning/NeuralTextures/train/',\n",
    "                                               'datasets/TransferLearning/FaceSwap/train/',\n",
    "                                               'datasets/TransferLearning/Face2Face/train/'],\n",
    "                                                      frames_per_video_per_batch,\n",
    "                                                       train_frames_per_video)\n",
    "\n",
    "X_val,y_val,class_weights_val=create_sequence(['datasets/DeepFake/val/',\n",
    "                                              'datasets/NeuralTextures/val/',\n",
    "                                              'datasets/FaceSwap/val/',\n",
    "                                              'datasets/Face2Face/val/'],\n",
    "                                              frames_per_video_per_batch,\n",
    "                                                    val_frames_per_video)\n",
    "\n",
    "train_it=DFVDSequence(X_train,y_train,train_video_per_batch,\n",
    "                      frames_per_video_per_batch,image_size,\n",
    "                      train_augumentation,True)\n",
    "val_it=DFVDSequence(X_val,y_val,val_video_per_batch,\n",
    "                    frames_per_video_per_batch,image_size,\n",
    "                    val_augumentation,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "model=CLRNet(input_shape=(frames_per_video_per_batch,image_size, image_size, channel), classes=2, block='bottleneck', residual_unit='v2',\n",
    "           repetitions=[3, 4, 6, 3], initial_filters=64, activation='softmax', include_top=False,\n",
    "           input_tensor=None, dropout=0.25, transition_dilation_rate=(1, 1),\n",
    "           initial_strides=(2, 2), initial_kernel_size=(7, 7), initial_pooling='max',\n",
    "           final_pooling=None, top='classification')\n",
    "model.load_weights('CLR_DF+NT_Ebest.h5')\n",
    "is_training=False\n",
    "top_k_layers=206\n",
    "model,df=FreezeBatchNormalization(is_training,top_k_layers,model)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(df)\n",
    "adam_fine = Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam_fine,\n",
    "                  metrics=['accuracy'])\n",
    "print_summary(model, line_length=150, positions=None, print_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frames_per_video_per_batch=5\n",
    "# frames_per_video=80\n",
    "# image_size=128\n",
    "# test_video_per_batch=50\n",
    "# data_augmentation=False\n",
    "# training=False\n",
    "# X_DeepFake,y_DeepFake,class_weights_DeepFake=create_sequence(['datasets/DeepFake/val/','datasets/NeuralTextures/val/'],frames_per_video_per_batch,frames_per_video)\n",
    "# DeepFake_it=DFVDSequence(X_DeepFake,y_DeepFake,test_video_per_batch,frames_per_video_per_batch,image_size,data_augmentation,training)\n",
    "# predictions = model.predict_generator(DeepFake_it, verbose=1)\n",
    "# predicted_classes = np.argmax(predictions, axis=1)\n",
    "# true_classes = DeepFake_it.classes\n",
    "# report = classification_report(true_classes, predicted_classes, digits=4)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfreezing Few Layers for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "is_training = True\n",
    "top_k_layers=206\n",
    "model,df=FreezeBatchNormalization(is_training,top_k_layers,model)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(df)\n",
    "adam_fine = Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "# adam_fine=Adam()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam_fine,\n",
    "                  metrics=['accuracy'])\n",
    "print_summary(model, line_length=150, positions=None, print_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=\"CLR_DF+NT-(FS,F2F,DFD,DFDC)_Ebest.h5\",\n",
    "    save_weights_only=True,\n",
    "    monitor='val_acc',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    \"CLR_DF+NT-(FS,F2F,DFD,DFDC)_train.csv\",\n",
    "    append=True,\n",
    "    separator=',')\n",
    "\n",
    "reduce_lr_loss = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=7,\n",
    "    verbose=1,\n",
    "    min_delta=0.0001,\n",
    "    mode='min')\n",
    "\n",
    "\n",
    "callbacks_list = [csv_logger,model_checkpoint_callback,reduce_lr_loss] \n",
    "\n",
    "# _id = dt.now().strftime(\"%y-%m-%d-%H_%M\")\n",
    "# save_dir=os.path.join(os.getcwd(),'models','transfer_FS-DF-F2F-NT-DFD','CLRNet','DF-F2F-NT-DFD',_id)\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "\n",
    "# filepath=os.path.join(save_dir,\"{epoch:02d}-{acc:.2f}\"+\".hdf5\")\n",
    "# checkpoint = ModelCheckpoint(filepath)#, monitor='acc', verbose=2, save_best_only=True, mode='max')\n",
    "# # csv_logger = CSVLogger(os.path.join(save_dir,\"training.csv\"), append=True, separator=',')\n",
    "# # earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "# # mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "# # reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "\n",
    "# # earlyStop=EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "# # callbacks_list = [csv_logger]\n",
    "\n",
    "# csv_logger = AdditionalValidationSets([(val_it_0,'valFS'),(val_it_1,'valDF'),(val_it_2,'valF2F'),(val_it_3,'valNT'),(val_it_4,'valDFD'),(test_it_0,'testFS'),(test_it_1,'testDF'),(test_it_2,'testF2F'),(test_it_3,'testNT'),(test_it_4,'testDFD')],os.path.join(save_dir,\"training.csv\"))\n",
    "# # csv_logger = AdditionalValidationSets([(val_it_0,'val0'),(val_it_1,'val1'),(val_it_2,'val2'),(val_it_3,'val3')],os.path.join(save_dir,\"training.csv\"))\n",
    "\n",
    "# callbacks_list = [csv_logger,checkpoint]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_it,\n",
    "                    epochs=150,\n",
    "                    validation_data=val_it,\n",
    "                    callbacks=callbacks_list,\n",
    "                    shuffle=False,\n",
    "                    class_weight=class_weights_train,\n",
    "                    verbose=1)\n",
    "model.save_weights(\"CLR_DF+NT-(FS,F2F,DFD,DFDC)_E150.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}