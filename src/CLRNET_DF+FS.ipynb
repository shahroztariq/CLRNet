{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CLRNET_DF+FS_train\n",
    "### Import Python Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.backend import clear_session\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.python.keras.utils.layer_utils import print_summary\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "import numpy as np\n",
    "from CLRNet_convlstm import CLRNet\n",
    "from Utility_functions import create_sequence, FreezeBatchNormalization,AdditionalValidationSets\n",
    "from datetime import datetime as dt\n",
    "from DFVDSequence import DFVDSequence\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"yolo\")\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "# import shutil\n",
    "from tensorflow.python.keras import layers\n",
    "# from src.xception_convlstm import Xception\n",
    "# print(cv2.__version__)\n",
    "# import random\n",
    "# random.seed(32)\n",
    "\n",
    "dataset_dir='DeepFakeDatasetReal'\n",
    "# from src.cl_basic import cl_basic\n",
    "import ipykernel\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# Choose GPU NUMBERS [0, 1, 2, 3]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "sess.run(tf.global_variables_initializer())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DF+FS Transfer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "train_video_per_batch=40\n",
    "val_video_per_batch=25\n",
    "test_video_per_batch=25\n",
    "train_frames_per_video=10 #total frames\n",
    "val_frames_per_video=5 #total frames\n",
    "test_frames_per_video=5 #total frames\n",
    "frames_per_video_per_batch=5 #frames in one batch\n",
    "image_size=128\n",
    "channel=3\n",
    "train_augumentation=True\n",
    "val_augumentation=False\n",
    "test_augumentation=False\n",
    "X_train,y_train, class_weights_train = create_sequence(['datasets/DeepFake/train/',\n",
    "                                               'datasets/FaceSwap/train/'],\n",
    "                                                      frames_per_video_per_batch,\n",
    "                                                       train_frames_per_video)\n",
    "\n",
    "X_val_0,y_val_0,class_weights_val_0=create_sequence(['datasets/DeepFake/val/'],\n",
    "                                              frames_per_video_per_batch,\n",
    "                                                    val_frames_per_video)\n",
    "X_val_1,y_val_1,class_weights_val_1=create_sequence(['datasets/FaceSwap/val/'],\n",
    "                                              frames_per_video_per_batch,\n",
    "                                                    val_frames_per_video)\n",
    "\n",
    "\n",
    "train_it=DFVDSequence(X_train,y_train,train_video_per_batch,\n",
    "                      frames_per_video_per_batch,image_size,\n",
    "                      train_augumentation,True)\n",
    "\n",
    "val_it_0=DFVDSequence(X_val_0,y_val_0,val_video_per_batch,\n",
    "                    frames_per_video_per_batch,image_size,\n",
    "                    val_augumentation,False)\n",
    "val_it_1=DFVDSequence(X_val_1,y_val_1,val_video_per_batch,\n",
    "                    frames_per_video_per_batch,image_size,\n",
    "                    val_augumentation,False)\n",
    "\n",
    "fig=plt.figure(figsize=(10, 100))\n",
    "columns = 4\n",
    "rows = 23\n",
    "print(\"Training\")\n",
    "x=1\n",
    "temp=train_it.__getitem__(1)\n",
    "fig.add_subplot(rows, columns, x)\n",
    "print(\"Min:\",np.array(temp[0][0,0]).min(),\"Max:\",np.array(temp[0][0,0]).max())\n",
    "print(\"Min:\",np.array(temp[0][1,0]).min(),\"Max:\",np.array(temp[0][1,0]).max())\n",
    "plt.imshow(np.array(temp[0][0,0]))\n",
    "fig.add_subplot(rows, columns, x+1)\n",
    "plt.imshow(np.array(temp[0][1,0]))\n",
    "print(temp[0].shape,temp[1].shape,train_it.dataset_size,train_it.frame_counter,train_it.__len__(),temp[1][0],temp[1][1])\n",
    "# x+=2\n",
    "plt.title(\"Training Set Examples\")        \n",
    "plt.show()\n",
    "train_it.on_epoch_end()\n",
    "\n",
    "fig=plt.figure(figsize=(10, 100))\n",
    "columns = 4\n",
    "rows = 23\n",
    "print(\"Validation - 0\")\n",
    "x=1\n",
    "temp=val_it_0.__getitem__(1)\n",
    "fig.add_subplot(rows, columns, x)\n",
    "print(\"Min:\",np.array(temp[0][0,0]).min(),\"Max:\",np.array(temp[0][0,0]).max())\n",
    "print(\"Min:\",np.array(temp[0][1,0]).min(),\"Max:\",np.array(temp[0][1,0]).max())\n",
    "plt.imshow(np.array(temp[0][0,0]),)\n",
    "fig.add_subplot(rows, columns, x+1)\n",
    "plt.imshow(np.array(temp[0][1,0]))\n",
    "print(temp[0].shape,temp[1].shape,val_it_0.dataset_size,val_it_0.frame_counter,val_it_0.__len__(),temp[1][0],temp[1][1])\n",
    "plt.title(\"Validation - 0 Set Examples\")   \n",
    "plt.show()\n",
    "val_it_0.on_epoch_end()\n",
    "\n",
    "fig=plt.figure(figsize=(10, 100))\n",
    "columns = 4\n",
    "rows = 23\n",
    "print(\"Validation - 1\")\n",
    "x=1\n",
    "temp=val_it_1.__getitem__(1)\n",
    "fig.add_subplot(rows, columns, x)\n",
    "print(\"Min:\",np.array(temp[0][0,0]).min(),\"Max:\",np.array(temp[0][0,0]).max())\n",
    "print(\"Min:\",np.array(temp[0][1,0]).min(),\"Max:\",np.array(temp[0][1,0]).max())\n",
    "plt.imshow(np.array(temp[0][0,0]),)\n",
    "fig.add_subplot(rows, columns, x+1)\n",
    "plt.imshow(np.array(temp[0][1,0]))\n",
    "print(temp[0].shape,temp[1].shape,val_it_1.dataset_size,val_it_1.frame_counter,val_it_1.__len__(),temp[1][0],temp[1][1])\n",
    "plt.title(\"Validation - 1 Set Examples\")   \n",
    "plt.show()\n",
    "val_it_1.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10, 100))\n",
    "columns = 5\n",
    "rows = 40\n",
    "print(\"ALL\")\n",
    "x=1\n",
    "temp=train_it.__getitem__(1)\n",
    "fig.add_subplot(rows, columns, x)\n",
    "plt.imshow(np.array(temp[0][0,0]))\n",
    "fig.add_subplot(rows, columns, x+1)\n",
    "plt.imshow(np.array(temp[0][0,1]))\n",
    "fig.add_subplot(rows, columns, x+2)\n",
    "plt.imshow(np.array(temp[0][0,2]))\n",
    "fig.add_subplot(rows, columns, x+3)\n",
    "plt.imshow(np.array(temp[0][0,3]))\n",
    "fig.add_subplot(rows, columns, x+4)\n",
    "plt.imshow(np.array(temp[0][0,4]))\n",
    "print(\"Min:\",np.array(temp[0][0,0]).min(),\"Max:\",np.array(temp[0][0,0]).max())\n",
    "fig.add_subplot(rows, columns, x+5)\n",
    "plt.imshow(np.array(temp[0][1,0]))\n",
    "fig.add_subplot(rows, columns, x+6)\n",
    "plt.imshow(np.array(temp[0][1,1]))\n",
    "fig.add_subplot(rows, columns, x+7)\n",
    "plt.imshow(np.array(temp[0][1,2]))\n",
    "fig.add_subplot(rows, columns, x+8)\n",
    "plt.imshow(np.array(temp[0][1,3]))\n",
    "fig.add_subplot(rows, columns, x+9)\n",
    "plt.imshow(np.array(temp[0][1,4]))\n",
    "print(\"Min:\",np.array(temp[0][1,0]).min(),\"Max:\",np.array(temp[0][1,0]).max())\n",
    "print(temp[0].shape,temp[1].shape,train_it.dataset_size,train_it.frame_counter,train_it.__len__(),temp[1][0],temp[1][1])\n",
    "plt.title(\"Training Set Examples\")        \n",
    "plt.show()\n",
    "train_it.on_epoch_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "# is_training = True\n",
    "# top_k_layers=120\n",
    "# model,df=FreezeBatchNormalization(is_training,top_k_layers,model)\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "#     display(df)\n",
    "model=CLRNet(input_shape=(frames_per_video_per_batch,image_size, image_size, channel), classes=2, block='bottleneck', residual_unit='v2',\n",
    "           repetitions=[3, 4, 6, 3], initial_filters=64, activation='softmax', include_top=False,\n",
    "           input_tensor=None, dropout=0.25, transition_dilation_rate=(1, 1),\n",
    "           initial_strides=(2, 2), initial_kernel_size=(7, 7), initial_pooling='max',\n",
    "           final_pooling=None, top='classification')\n",
    "# print_summary(model, line_length=150, positions=None, print_fn=None)\n",
    "adam_fine = Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam_fine,\n",
    "                  metrics=['accuracy'])\n",
    "print_summary(model, line_length=150, positions=None, print_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "_id = dt.now().strftime(\"%y-%m-%d-%H_%M\")\n",
    "save_dir=os.path.join('/media/data1/sha/','models','CLRNET_DF+FS','CLRNet','DF+FS',_id)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath=os.path.join(save_dir,\"{epoch:02d}-{acc:.2f}\"+\".hdf5\")\n",
    "checkpoint = ModelCheckpoint(filepath)#, monitor='acc', verbose=2, save_best_only=True, mode='max')\n",
    "csv_logger = AdditionalValidationSets([(val_it_0,'valDF'),(val_it_1,'valFS')],os.path.join(save_dir,\"training.csv\"))\n",
    "callbacks_list = [csv_logger,checkpoint]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_it,epochs=150,\n",
    "                    callbacks=callbacks_list,shuffle=False,class_weight=class_weights_train,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('/media/data1/sha/models/CLRNET_DF+FS/CLRNet/DF+FS/20-05-28-08_24/98-0.80.hdf5')\n",
    "model.fit_generator(generator=train_it,epochs=150, initial_epoch=98,\n",
    "                    callbacks=callbacks_list,shuffle=False,class_weight=class_weights_train,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}