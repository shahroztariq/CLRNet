{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# transfer_NT-DF_train\n",
    "### Import Python Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.backend import clear_session\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.python.keras.utils.layer_utils import print_summary\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.models import load_model\n",
    "import numpy as np\n",
    "from CLRNet_convlstm import CLRNet\n",
    "from Utility_functions import create_sequence, FreezeBatchNormalization,AdditionalValidationSets\n",
    "from datetime import datetime as dt\n",
    "from DFVDSequence import DFVDSequence\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"yolo\")\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "# import shutil\n",
    "from tensorflow.python.keras import layers\n",
    "# from src.xception_convlstm import Xception\n",
    "# print(cv2.__version__)\n",
    "# import random\n",
    "# random.seed(32)\n",
    "\n",
    "dataset_dir='DeepFakeDatasetReal'\n",
    "# from src.cl_basic import cl_basic\n",
    "import ipykernel\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# Choose GPU NUMBERS [0, 1, 2, 3]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NT-DF Transfer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "train_video_per_batch=16\n",
    "val_video_per_batch=20\n",
    "train_frames_per_video=80 #total frames\n",
    "val_frames_per_video=10 #total frames\n",
    "frames_per_video_per_batch=5 #frames in one batch\n",
    "image_size=240\n",
    "channel=3\n",
    "train_augumentation=True\n",
    "val_augumentation=False\n",
    "\n",
    "X_train,y_train, class_weights_train = create_sequence(['datasets/TransferLearning/NeuralTextures/train/',\n",
    "                                               'datasets/TransferLearning/DeepFake/train/'],\n",
    "                                                      frames_per_video_per_batch,\n",
    "                                                       train_frames_per_video)\n",
    "X_val_0,y_val_0,class_weights_val_0=create_sequence(['datasets/NeuralTextures/val/'],\n",
    "                                              frames_per_video_per_batch,\n",
    "                                                    val_frames_per_video)\n",
    "\n",
    "X_val_1,y_val_1,class_weights_val_1=create_sequence(['datasets/DeepFake/val/'],\n",
    "                                              frames_per_video_per_batch,\n",
    "                                                    val_frames_per_video)\n",
    "\n",
    "\n",
    "train_it=DFVDSequence(X_train,y_train,train_video_per_batch,\n",
    "                      frames_per_video_per_batch,image_size,\n",
    "                      train_augumentation,True)\n",
    "val_it_0=DFVDSequence(X_val_0,y_val_0,val_video_per_batch,\n",
    "                    frames_per_video_per_batch,image_size,\n",
    "                    val_augumentation,False)\n",
    "val_it_1=DFVDSequence(X_val_1,y_val_1,val_video_per_batch,\n",
    "                    frames_per_video_per_batch,image_size,\n",
    "                    val_augumentation,False)\n",
    "\n",
    "fig=plt.figure(figsize=(10, 100))\n",
    "columns = 4\n",
    "rows = 23\n",
    "print(\"Training\")\n",
    "x=1\n",
    "temp=train_it.__getitem__(1)\n",
    "fig.add_subplot(rows, columns, x)\n",
    "print(\"Min:\",np.array(temp[0][0,0]).min(),\"Max:\",np.array(temp[0][0,0]).max())\n",
    "print(\"Min:\",np.array(temp[0][1,0]).min(),\"Max:\",np.array(temp[0][1,0]).max())\n",
    "plt.imshow(np.array(temp[0][0,0]))\n",
    "fig.add_subplot(rows, columns, x+1)\n",
    "plt.imshow(np.array(temp[0][1,0]))\n",
    "print(temp[0].shape,temp[1].shape,train_it.dataset_size,train_it.frame_counter,train_it.__len__(),temp[1][0],temp[1][1])\n",
    "# x+=2\n",
    "plt.title(\"Training Set Examples\")        \n",
    "plt.show()\n",
    "train_it.on_epoch_end()\n",
    "\n",
    "fig=plt.figure(figsize=(10, 100))\n",
    "columns = 4\n",
    "rows = 23\n",
    "print(\"Validation - 0\")\n",
    "x=1\n",
    "temp=val_it_0.__getitem__(1)\n",
    "fig.add_subplot(rows, columns, x)\n",
    "print(\"Min:\",np.array(temp[0][0,0]).min(),\"Max:\",np.array(temp[0][0,0]).max())\n",
    "print(\"Min:\",np.array(temp[0][1,0]).min(),\"Max:\",np.array(temp[0][1,0]).max())\n",
    "plt.imshow(np.array(temp[0][0,0]),)\n",
    "fig.add_subplot(rows, columns, x+1)\n",
    "plt.imshow(np.array(temp[0][1,0]))\n",
    "print(temp[0].shape,temp[1].shape,val_it_0.dataset_size,val_it_0.frame_counter,val_it_0.__len__(),temp[1][0],temp[1][1])\n",
    "plt.title(\"Validation - 0 Set Examples\")   \n",
    "plt.show()\n",
    "val_it_0.on_epoch_end()\n",
    "\n",
    "fig=plt.figure(figsize=(10, 100))\n",
    "columns = 4\n",
    "rows = 23\n",
    "print(\"Validation - 1\")\n",
    "x=1\n",
    "temp=val_it_1.__getitem__(1)\n",
    "fig.add_subplot(rows, columns, x)\n",
    "print(\"Min:\",np.array(temp[0][0,0]).min(),\"Max:\",np.array(temp[0][0,0]).max())\n",
    "print(\"Min:\",np.array(temp[0][1,0]).min(),\"Max:\",np.array(temp[0][1,0]).max())\n",
    "plt.imshow(np.array(temp[0][0,0]),)\n",
    "fig.add_subplot(rows, columns, x+1)\n",
    "plt.imshow(np.array(temp[0][1,0]))\n",
    "print(temp[0].shape,temp[1].shape,val_it_1.dataset_size,val_it_1.frame_counter,val_it_1.__len__(),temp[1][0],temp[1][1])\n",
    "plt.title(\"Validation - 1 Set Examples\")   \n",
    "plt.show()\n",
    "val_it_1.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10, 100))\n",
    "columns = 5\n",
    "rows = 40\n",
    "print(\"NT-DF\")\n",
    "x=1\n",
    "temp=train_it.__getitem__(1)\n",
    "fig.add_subplot(rows, columns, x)\n",
    "plt.imshow(np.array(temp[0][0,0]))\n",
    "fig.add_subplot(rows, columns, x+1)\n",
    "plt.imshow(np.array(temp[0][0,1]))\n",
    "fig.add_subplot(rows, columns, x+2)\n",
    "plt.imshow(np.array(temp[0][0,2]))\n",
    "fig.add_subplot(rows, columns, x+3)\n",
    "plt.imshow(np.array(temp[0][0,3]))\n",
    "fig.add_subplot(rows, columns, x+4)\n",
    "plt.imshow(np.array(temp[0][0,4]))\n",
    "print(\"Min:\",np.array(temp[0][0,0]).min(),\"Max:\",np.array(temp[0][0,0]).max())\n",
    "fig.add_subplot(rows, columns, x+5)\n",
    "plt.imshow(np.array(temp[0][3,0]))\n",
    "fig.add_subplot(rows, columns, x+6)\n",
    "plt.imshow(np.array(temp[0][3,1]))\n",
    "fig.add_subplot(rows, columns, x+7)\n",
    "plt.imshow(np.array(temp[0][3,2]))\n",
    "fig.add_subplot(rows, columns, x+8)\n",
    "plt.imshow(np.array(temp[0][3,3]))\n",
    "fig.add_subplot(rows, columns, x+9)\n",
    "plt.imshow(np.array(temp[0][3,4]))\n",
    "print(\"Min:\",np.array(temp[0][1,0]).min(),\"Max:\",np.array(temp[0][1,0]).max())\n",
    "print(temp[0].shape,temp[1].shape,train_it.dataset_size,train_it.frame_counter,train_it.__len__(),temp[1][0],temp[1][3])\n",
    "plt.title(\"Training Set Examples\")        \n",
    "plt.show()\n",
    "train_it.on_epoch_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model('models/NeuralTexture/CLRNet50(bk)/20-01-07-07_22/96-1.00.hdf5')\n",
    "is_training=False\n",
    "top_k_layers=120\n",
    "model,df=FreezeBatchNormalization(is_training,top_k_layers,model)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(df)\n",
    "adam_fine = Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam_fine,\n",
    "                  metrics=['accuracy'])\n",
    "print_summary(model, line_length=150, positions=None, print_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_per_video_per_batch=5\n",
    "frames_per_video=80\n",
    "image_size=240\n",
    "test_video_per_batch=16\n",
    "data_augmentation=False\n",
    "training=False\n",
    "X_DeepFake,y_DeepFake,class_weights_DeepFake=create_sequence(['datasets/NeuralTextures/val/',],frames_per_video_per_batch,frames_per_video)\n",
    "DeepFake_it=DFVDSequence(X_DeepFake,y_DeepFake,test_video_per_batch,frames_per_video_per_batch,image_size,data_augmentation,training)\n",
    "predictions = model.predict_generator(DeepFake_it, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = DeepFake_it.classes\n",
    "report = classification_report(true_classes, predicted_classes, digits=4)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfreezing Few Layers for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "is_training = True\n",
    "top_k_layers=120\n",
    "model,df=FreezeBatchNormalization(is_training,top_k_layers,model)\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(df)\n",
    "adam_fine = Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam_fine,\n",
    "                  metrics=['accuracy'])\n",
    "print_summary(model, line_length=150, positions=None, print_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "_id = dt.now().strftime(\"%y-%m-%d-%H_%M\")\n",
    "save_dir=os.path.join(os.getcwd(),'models','transfer_NT-DF','CLRNet','DF',_id)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "filepath=os.path.join(save_dir,\"{epoch:02d}-{acc:.2f}\"+\".hdf5\")\n",
    "checkpoint = ModelCheckpoint(filepath)#, monitor='acc', verbose=2, save_best_only=True, mode='max')\n",
    "# csv_logger = CSVLogger(os.path.join(save_dir,\"training.csv\"), append=True, separator=',')\n",
    "# earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "# mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "# reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "\n",
    "# earlyStop=EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "# callbacks_list = [csv_logger]\n",
    "\n",
    "csv_logger = AdditionalValidationSets([(val_it_0,'val0'),(val_it_1,'val1')],os.path.join(save_dir,\"training.csv\"))\n",
    "callbacks_list = [csv_logger,checkpoint]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=train_it,epochs=150,\n",
    "                    callbacks=callbacks_list,shuffle=False,class_weight=class_weights_train,verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}